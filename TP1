import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import nltk
# Chargement des données depuis le fichier CSV téléchargé depuis Kaggle
data = pd.read_csv('/kaggle/input/customer-support-on-twitter/sample.csv')

# Téléchargement des stopwords en anglais

nltk.download('stopwords')
#téléchargement des wordnet en  anglais 
nltk.download('wordnet')

# Initialisation du stemmer de Porter
stemming = PorterStemmer()

# Fonction de nettoyage du texte
def clean_text(text):
    # Mise en minuscules
    text = text.lower()
    # Suppression des ponctuations
    text = re.sub(r'[^\w\s]', '', text)
      # Suppression des émojis et des émoticônes
    text = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002702-\U000027B0\U000024C2-\U0001F251]+', '', text)
    # Suppression des URL
    text = re.sub(r'http\S+|www\S+', '', text)
    # Suppression des balises HTML
    text = re.sub(r'<.*?>', '', text)
    return text

# Appliquer la fonction de nettoyage sur la colonne 'text'
data['text_cleaned'] = data['text'].apply(clean_text)

#  suppression des mots vides
stop_words = set(stopwords.words('english'))
data['text_cleaned'] = data['text_cleaned'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))

# Appliquer le stemming sur la colonne 'text_cleane'
data['text_stemmed'] = data['text_cleaned'].apply(lambda x: ' '.join([stemming.stem(word) for word in word_tokenize(x)]))

# Lemmatisation des mots
#lemmatizer = WordNetLemmatizer()
#data['text_cleaned'] = data['text_cleaned'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))

# Affichage du Data avec les colonnes 'tweet_id', 'text', 'text_cleane' et 'text_stemme'
cleaned_data = data[['tweet_id', 'text', 'text_cleaned', 'text_stemmed']]
print(cleaned_data.head())
